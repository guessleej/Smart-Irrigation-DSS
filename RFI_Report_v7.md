# 財政部財政資訊中心

## 「智慧算力共用平臺建置委外服務案」

### 公開徵求資訊 (Request for Information, RFI) 回覆說明文件

**提案廠商：云碩科技 (xCloudinfo Group)**

**文件版本：7.0**

**日期：2026 年 2 月 13 日**

---

## 摘要

本文件旨在回應財政部財政資訊中心（以下簡稱「貴中心」）針對「智慧算力共用平臺建置委外服務案」所提出之公開徵求資訊 (RFI)。云碩科技（xCloudinfo Group）深刻理解本專案不僅是為了解決當前生成式人工智慧 (Generative AI) 應用所需之算力缺口，更是為貴中心未來十年 AI 發展藍圖奠定堅實、安全且具前瞻性的數位基礎設施。面對高效能算力需求、封閉式地端環境的嚴苛資安要求、以及資源統籌管理的複雜性，本文件提出一套全棧式 (Full-Stack)、端到端的 AI 平台解決方案。

本方案以貴中心內部機房建置為首選，在完全符合機房功率、散熱、承重等限制條件下，規劃了以 **NVIDIA H200 NVL 141GB** 與 **RTX 6000 Pro Server Blackwell Edition 96GB** 混合配置的 GPU 運算叢集、**NetApp AFF A50 HA Pair** 全快閃儲存陣列（**80TB** 可用空間）、基於 **NVIDIA SN3700C / SN3420 Leaf-Spine** 架構的高效能網路，以及 **Palo Alto PA-5450** 防火牆與 **A10 Networks Thunder ADC** 組成的資安邊界防護。在軟體層面，我們提出以 **SEGMA 平台**為核心的雙層架構：上層是賦能業務創新的 **SEGMA AI 服務平台 (SaaS)**，提供 Agentic RAG、資料工作室等模組化 AI 功能；底層則是負責硬體資源管理的 **SEGMA 資源管理平台 (PaaS)**，整合了先進的 GPU 虛擬化、智慧調度與內建 OpenAI 相容的 **SEGMA API Gateway**，提供從 IaaS 到 SaaS 端到端的整合體驗。此外，本方案更創新性地導入 **8 台研華 MIC-743-AT 邊緣運算單元**，搭載 NVIDIA Jetson Thor 處理器，部署於各業務單位，作為本地知識庫引擎與邊緣推論節點，實現「中心算力 + 邊緣智慧」的雙層 AI 架構。

本文件將依循專業論文體例，從專案背景、系統架構、各子系統設計細節，到 CPU 與記憶體的精確配置、專案管理規劃與成本效益分析，進行全面且深入的闡述，以期完整呈現云碩科技對本專案的專業洞察與卓越執行能力。

---

## 目錄

- **第一章 緒論**
  - 1.1 研究背景與動機
  - 1.2 專案目標與核心挑戰
  - 1.3 方案範疇與文件結構
- **第二章 系統總體架構設計**
  - 2.1 設計原則
  - 2.2 四層式系統架構
  - 2.3 部署模式：內部機房建置方案
- **第三章 基礎設施層 (IaaS) 設計**
  - 3.1 運算子系統 (Compute Subsystem)
  - 3.2 儲存子系統 (Storage Subsystem)
  - 3.3 網路子系統 (Network Subsystem)
  - 3.4 資安邊界子系統 (Security Perimeter Subsystem)
  - 3.5 邊緣運算子系統 (Edge Computing Subsystem)
- **第四章 SEGMA AI 服務平台 (SaaS) 設計**
  - 4.1 平台定位：企業級 AI 數據中台
  - 4.2 Agentic RAG 核心工作流
  - 4.3 模組化功能設計
  - 4.4 中文化圖形化介面
- **第五章 服務與管理層 (PaaS) 設計**
  - 5.1 SEGMA 資源管理平台總覽
  - 5.2 GPU 資源管理與智慧調度機制
  - 5.3 模型部署與高效推論引擎
  - 5.4 SEGMA API Gateway：統一模型服務供應
  - 5.5 系統監控與可觀測性
  - 5.6 使用者、權限與叢集管理
- **第六章 專案管理與執行規劃**
  - 6.1 專案建置時程規劃
  - 6.2 預算成本估算
  - 6.3 風險評估與應對策略
- **第七章 結論與展望**
- **附錄 A：伺服器詳細配置表**
- **參考文獻**

---

## 第一章 緒論

### 1.1 研究背景與動機

隨著生成式人工智慧技術的飛速發展，大型語言模型 (LLM)、語音辨識 (ASR) 及檢索增強生成 (RAG) 等應用已成為推動政府數位轉型、提升公共服務品質的關鍵動能。財政部財政資訊中心作為國家財政數據中樞，肩負著處理大量機敏資料的重責大任，其 AI 應用的發展不僅追求效率，更需將資訊安全與資料自主性置於首位。然而，現有資訊環境普遍面臨 AI 算力不足、資源管理分散、以及缺乏符合 A 級機關資安規範的地端 AI 基礎設施等問題。為此，貴中心提出「智慧算力共用平臺建置委外服務案」，旨在建構一個高效、安全、可控的 AI 算力基礎設施 (IaaS)，以滿足日益增長的 AI 應用需求，並為未來的智慧財政創新奠定基石。

### 1.2 專案目標與核心挑戰

根據 RFI 文件內容，本專案的核心目標在於建置或租用一套高效能的智慧算力基礎設施，以整合並擴充其內網的生成式 AI 應用能力。我們將此目標解構成以下五大核心挑戰：

1. **高效能算力挑戰**：平台需提供足以支撐多種大型模型同時運作，並滿足嚴苛回應效率指標（TTFT < 5-10 秒，TPS > 20）的高強度 GPU 算力。
2. **極致安全挑戰**：由於涉及機敏資料，整體解決方案必須在與網際網路實體隔離的「封閉式地端」環境中部署，對廠商的資安建置、實體隔離與離線維運能力構成極高要求。
3. **精細化管理挑戰**：需建置一套功能完善的資源管理平台，具備資源池化、虛擬化切割 (vGPU)、自動化調度、多租戶權限控管及精細化監控報表等能力，以最大化資源利用率並簡化管理複雜度。
4. **部署彈性挑戰**：方案需考量於貴中心內部機房建置、外部機房租用，或直接租用算力服務等不同模式，考驗廠商整合不同部署模式與環境限制的能力。
5. **邊緣智慧挑戰**：各業務單位對於本地化知識庫與低延遲推論有迫切需求，如何在中心化算力平台之外，延伸 AI 能力至各單位端點，是本方案的創新挑戰。

### 1.3 方案範疇與文件結構

為完整回應上述挑戰，本文件將提出一套涵蓋硬體基礎設施、軟體平台服務、邊緣運算延伸、專案管理與後續維運的「全棧式 AI 平台解決方案」。文件結構安排如下：第二章闡述系統的總體架構與設計原則。第三章深入探討基礎設施層的各個子系統設計細節，包含運算、儲存、網路、資安邊界及邊緣運算。第四章說明服務管理層的軟體平台設計。第五章提出具體的專案執行時程、成本估算與風險管理策略。最後，第六章進行總結與展望。

---

## 第二章 系統總體架構設計

### 2.1 設計原則

本方案的架構設計遵循以下六大核心原則：

1. **安全優先 (Security-First)**：所有設計均以滿足 A 級機關的資安規範為最高前提，採用實體隔離、縱深防禦的策略，從資安邊界層的防火牆與 WAF 到內部網路的微分段，層層把關。
2. **效能導向 (Performance-Oriented)**：選用業界頂尖的硬體與軟體，針對 AI 工作負載進行優化，確保滿足 RFI 的效能指標。
3. **開放標準 (Open Standards)**：優先採用開源或相容開放標準的技術 (如 Kubernetes, OpenAI API)，避免廠商鎖定，提升系統的互通性與未來擴充性。
4. **自動化維運 (Automated Operations)**：透過軟體定義與自動化流程，簡化資源管理、模型部署與系統監控的複雜度，降低人力維運成本。
5. **模組化擴充 (Modular Scalability)**：採用模組化、可水平擴展的架構，確保未來無論是運算、儲存或網路資源，皆可獨立且平滑地進行擴充。
6. **中心與邊緣協同 (Center-Edge Synergy)**：創新性地結合中心機房的高效能算力與各單位部署的邊緣運算節點，實現「集中訓練、分散推論」的雙層智慧架構。

### 2.2 四層式系統架構

我們提出的「全棧式 AI 平台」採用分層但緊密整合的設計，由外至內、由上至下分為**資安邊界層、應用與開發層、服務與管理層、基礎設施層**四大核心層級，並延伸出**邊緣運算層**。此架構確保了各層級的獨立性與專業性，同時透過標準化的 API 進行高效協作。

**圖 2-1：智慧算力共用平臺總體架構圖 (v6.0)**

![智慧算力共用平臺總體架構圖](/home/ubuntu/architecture_diagram_v9.png)

如上圖所示，四層架構的職責劃分如下：

#### 2.2.1 資安邊界層 (Security Perimeter Layer)

資安邊界層是整體平台的第一道防線，部署於外部網路/內部網路與平台核心之間，採用**縱深防禦 (Defense in Depth)** 策略，由兩道關卡組成：

**第一道關卡：Palo Alto Networks PA-5450 防火牆 x2 (HA Pair)**。作為網路層的守門員，執行 L3/L4 封包過濾、入侵偵測與防禦 (IDS/IPS)、應用程式識別與控制等功能。兩台設備組成主動-被動 (Active-Passive) 高可用叢集，確保任一設備故障時服務不中斷。

**第二道關卡：A10 Networks Thunder ADC x2 (HA Pair)**。部署於防火牆之後，作為應用層的智慧閘道，負責以下關鍵功能：
- **URL 分流 (URL Routing)**：根據 API 請求的 URL 路徑，將流量智慧地導向不同的後端服務（如 LLM 推論、RAG 查詢、ASR 語音辨識等）。
- **WAF 保護 (Web Application Firewall)**：針對 OWASP Top 10 等常見 Web 攻擊提供即時防護，保護後端 API 服務免受 SQL Injection、XSS 等攻擊。
- **負載平衡 (Load Balancing)**：將 API 請求均勻分配至多台後端伺服器，確保服務的高可用性與最佳回應時間。
- **SSL/TLS 卸載 (SSL Offloading)**：在 ADC 層完成加解密運算，減輕後端伺服器的運算負擔。

#### 2.2.2 應用與開發層 (Application & Development Layer)

應用與開發層是平台的最終使用者介面，包含**專案開發者**與各式 **AI 應用服務**（如 RAG、ASR 等）。開發者透過平台核發的 **API Key** 存取平台提供的 AI 服務，所有 API 請求均經由資安邊界層的防火牆與 A10 ADC 進行安全過濾與智慧分流後，方可抵達後端的服務與管理層。

#### 2.2.3 服務與管理層 (Service & Management Layer)

服務與管理層為平台的大腦與中樞，負責資源的虛擬化、調度、管理、監控，並將底層硬體資源封裝成標準化的 AI 服務。此層以 **SEGMA 資源管理平台** 為核心，提供一站式的 GPU 資源管理解決方案。其主要元件與功能如下：
- **SEGMA 資源管理平台**：基於 Kubernetes 的雲原生架構，提供圖形化管理介面，實現 GPU 資源的池化、多租戶管理、vGPU 虛擬化切割、以及全自動化的智慧調度。平台支援多叢集管理，可統一納管地端與未來可能的雲端 GPU 資源。
- **SEGMA API Gateway**：平台內建的 API 閘道，完全相容 OpenAI API 標準。負責所有 AI 服務的 API Key 認證、請求路由、用量配額與速率限制管理，提供統一、安全的服務入口。
- **高效推論引擎 (Inference Engines)**：原生整合 **vLLM**、**SGLang**、**TensorRT-LLM** 等多種業界頂尖推論引擎，並可透過自訂後端功能進行擴充。平台會根據模型特性與效能需求，自動選擇最適合的引擎執行推論。
- **監控與告警平台 (Observability Stack)**：深度整合 **Grafana**、**Prometheus** 與 **Loki**，並透過 **NVIDIA DCGM Exporter** 蒐集詳細的 GPU 指標，提供從平台、叢集、節點到單張 GPU 的全維度監控儀表板與告警機制。
- **企業級向量資料庫 (Vector Database)**：整合 **Milvus** 與 **Weaviate**，提供高效能、可擴展的向量檢索服務，作為 RAG 應用的核心基礎。

#### 2.2.4 基礎設施層 (Infrastructure Layer)

基礎設施層作為平台的基石，提供所有硬體資源，由三大單元組成：
- **運算單元**：採用 **HPE ProLiant DL380a Gen12** 伺服器，搭載 **12 張 NVIDIA H200 NVL 141GB** 與 **6 張 NVIDIA RTX 6000 Pro Server Blackwell Edition 96GB** GPU，提供超越 RFI 要求的強大算力。每台伺服器配置 **NVIDIA ConnectX-7 200GbE** (GPU 高速互聯) 與 **ConnectX-6 Lx 25GbE** (管理/儲存) 雙網卡。
- **儲存單元**：採用 **NetApp AFF A50 HA Pair** 全快閃儲存陣列，提供 **80TB** 可用空間與微秒級超低延遲。
- **網路單元**：採用 **NVIDIA SN3700C x2** (Spine) 與 **NVIDIA SN3420 x3** (Leaf) 組成 Leaf-Spine 高效能網路架構。

#### 2.2.5 邊緣運算層 (Edge Computing Layer)

作為本方案的創新亮點，我們規劃部署 **8 台研華 MIC-743-AT 邊緣運算單元**於各業務單位。每台搭載 **NVIDIA Jetson Thor T5000** 處理器（2,070 TOPS FP4 算力、128GB LPDDR5X 記憶體），透過 **100GbE QSFP28 DAC** 線纜連接至 Leaf 交換器 (SN3420)，與中心平台實現高速資料同步。此設計使各單位能在本地執行知識庫查詢與輕量級推論，無需將敏感資料上傳至中心，兼顧了效能與資安。

### 2.3 部署模式：內部機房建置方案

經審慎評估 RFI 中對於「封閉式地端」的嚴格定義以及資料機敏性的高度要求，我們強烈建議並規劃將整體平台**建置於貴中心內部機房**。此方案雖面臨較多環境限制，但能最大程度地確保資料的實體安全與自主可控性，完全杜絕外部連線的風險，是唯一能完全符合 A 級機關最高資安標準的選項。我們已針對貴中心機房的環境限制（10kW/機櫃、無液冷、800kg/m² 載重、PUE<1.6）進行了詳盡的分析與規劃，後續章節將有詳細說明。

---

## 第三章 基礎設施層 (IaaS) 設計

基礎設施層是整個 AI 平台效能與穩定性的根本。我們的設計目標是在貴中心機房的環境限制下，打造一個效能頂尖、穩定可靠且易於擴充的硬體基礎。

### 3.1 運算子系統 (Compute Subsystem)

#### 3.1.1 算力需求分析

根據 RFI 文件表 5，本專案所需最低 GPU 總算力等同於 16 張 NVIDIA H100 SXM 等級 GPU 的總和。為滿足此需求並符合氣冷機房的功耗限制，我們提出兼具效能與能源效率的混合配置方案。

#### 3.1.2 GPU 配置方案

本方案採用 **NVIDIA H200 NVL 141GB PCIe** 與 **NVIDIA RTX 6000 Pro Server Blackwell Edition 96GB** 雙旗艦 GPU 混合配置策略，並依據不同工作負載特性進行分工：

**表 3-1：GPU 配置總覽**

| GPU 型號 | 數量 | 單卡 VRAM | 總 VRAM | 主要用途 |
|:---|:---:|:---:|:---:|:---|
| NVIDIA H200 NVL 141GB PCIe | 12 張 | 141 GB | 1,692 GB | 大型語言模型推論、模型微調、高精度運算 |
| NVIDIA RTX 6000 Pro Server Blackwell Edition 96GB | 6 張 | 96 GB | 576 GB | 中型模型推論、RAG 應用、多模態 AI |
| **合計** | **18 張** | — | **2,268 GB** | — |

**方案優勢：**

1. **效能超越**：H200 作為 H100 的升級版，提供 1.4 倍的記憶體頻寬提升，此組合在 FP32 與 FP16 精度下均大幅超越 RFI 算力要求。
2. **VRAM 優勢**：總 VRAM 高達 **2,268 GB**，能從容應對多個大型模型（如 Llama 3.1 70B、TAIDE-LX-7B 等）同時運行的需求。
3. **架構前瞻**：引入 H200 (Hopper) 與 RTX 6000 Blackwell 兩代最新架構 GPU，確保技術領先性與投資保護。
4. **功耗可控**：選用的 PCIe 版本 GPU 功耗較低，完全適用於氣冷環境。

**表 3-2：GPU 算力驗證表**

| 精度 | H200 PCIe (12 張) | RTX 6000 Blackwell (6 張) | 總算力 (TFLOPS) | RFI 需求算力 (TFLOPS) | 是否達標 |
|:---|:---:|:---:|:---:|:---:|:---:|
| FP32 | 792 | 756 | **1,548** | 1,072 | ✅ 超越 44% |
| FP16 | 23,664 | ~12,000 (估) | **~35,664** | 20,912 | ✅ 超越 70% |

#### 3.1.3 伺服器選型與配置

所有 GPU 伺服器統一採用 **HPE ProLiant DL380a Gen12 (4U)** 機架式伺服器。此型號是 HPE 專為 AI 工作負載設計的旗艦平台，支援最多 8 張雙寬 GPU，具備卓越的散熱設計與擴充能力。

**表 3-3：伺服器配置總覽**

| 伺服器角色 | GPU 配置 | 數量 | 每台 GPU 數 | CPU / 記憶體 |
|:---|:---|:---:|:---:|:---|
| H200 運算伺服器 | NVIDIA H200 NVL 141GB | 3 台 | 4 張 | 2x Intel Xeon 6 6745P / 1.5 TB |
| RTX 6000 運算伺服器 (4-GPU) | RTX 6000 Blackwell 96GB | 1 台 | 4 張 | 2x Intel Xeon 6 6745P / 1.5 TB |
| RTX 6000 運算伺服器 (2-GPU) | RTX 6000 Blackwell 96GB | 1 台 | 2 張 | 2x Intel Xeon 6 6527P / 512 GB |
| **合計** | — | **5 台** | **18 張** | — |

#### 3.1.4 伺服器詳細網卡配置

網卡的選型是確保 GPU 叢集效能的關鍵環節。根據 HPE 官方技術文件，不同的 GPU 數量配置對應不同的 PCIe 插槽規則與網卡安裝方式。以下分別說明：

**（一）4-GPU 配置伺服器（H200 伺服器 x3 + RTX 6000 伺服器 x1）**

在 4-GPU 配置下，DL380a Gen12 的 GPU Cage 內建專為 SmartNIC 設計的 PCIe 插槽，可安裝高階的 ConnectX-7 網卡，實現 GPU 直連高速網路。

**表 3-4：4-GPU 伺服器網卡配置**

| 用途 | 推薦網卡型號 | HPE 料號 | 速度 | 安裝位置 | 連接對象 |
|:---|:---|:---|:---|:---|:---|
| GPU 高速互聯 | NVIDIA ConnectX-7 | **P65333-B21** | 200GbE 雙埠 QSFP112 | GPU Cage SmartNIC 插槽 | Spine: SN3700C |
| 管理/儲存網路 | NVIDIA ConnectX-6 Lx | **P42044-B21** | 25GbE 雙埠 SFP28 | Riser Cage Slot 3 | Leaf: SN3420 |
| OCP 管理網路 | NVIDIA ConnectX-6 Lx OCP | **P42041-B21** | 25GbE 雙埠 SFP28 | OCP 3.0 Slot A | Leaf: SN3420 |

**表 3-5：4-GPU 伺服器 PCIe 插槽分配**

| PCIe 插槽 | 安裝元件 |
|:---|:---|
| GPU Cage Slot 13, 15, 17, 19 | 4x GPU (H200 或 RTX 6000 Blackwell) |
| GPU Cage NIC Slot 1 | 1x ConnectX-7 200GbE SmartNIC |
| Riser Cage Slot 3 | 1x ConnectX-6 Lx 25GbE |
| OCP 3.0 Slot A | 1x ConnectX-6 Lx 25GbE OCP |

**（二）2-GPU 配置伺服器（RTX 6000 伺服器 x1）**

在 2-GPU 配置下，GPU Cage 使用 captive riser 架構，SmartNIC 插槽不可用。因此，所有 NIC 安裝於後方 Riser Cage 與 OCP 插槽。由於僅有 2 張 GPU，100GbE 頻寬已綽綽有餘。

**表 3-6：2-GPU 伺服器網卡配置**

| 用途 | 推薦網卡型號 | HPE 料號 | 速度 | 安裝位置 | 連接對象 |
|:---|:---|:---|:---|:---|:---|
| GPU 高速互聯 | NVIDIA ConnectX-6 Dx | **P25960-B21** | 100GbE 雙埠 QSFP56 | Riser Cage Slot 1 | Spine: SN3700C |
| 管理/儲存網路 | NVIDIA ConnectX-6 Lx OCP | **P42041-B21** | 25GbE 雙埠 SFP28 | OCP 3.0 Slot A | Leaf: SN3420 |

**表 3-7：2-GPU 伺服器 PCIe 插槽分配**

| PCIe 插槽 | 安裝元件 |
|:---|:---|
| GPU Cage Slot 15, 17 | 2x RTX 6000 Pro Server Blackwell 96GB |
| Riser Cage Slot 1 | 1x ConnectX-6 Dx 100GbE |
| OCP 3.0 Slot A | 1x ConnectX-6 Lx 25GbE OCP |
| Riser Cage Slot 2, 3 | **空閒（可供未來擴充）** |

#### 3.1.5 機櫃配置規劃

為符合單機櫃 10kW 功率上限，我們規劃將 5 台伺服器分散至多個運算機櫃。每台 4-GPU 伺服器滿載功耗約 3.5-4.0 kW（含 CPU、記憶體、風扇等），2-GPU 伺服器約 2.0-2.5 kW，加上網路設備後，每個機櫃總功耗均控制在 10kW 以內。

### 3.2 儲存子系統 (Storage Subsystem)

#### 3.2.1 儲存需求分析

AI 平台的儲存系統需同時滿足以下需求：高吞吐量的模型檔案讀取（單一大型模型可達數十 GB）、低延遲的向量資料庫查詢、以及大容量的訓練資料與日誌儲存。

#### 3.2.2 儲存設備選型

**表 3-8：NetApp AFF A50 儲存配置**

| 項目 | 規格 |
|:---|:---|
| 推薦型號 | **NetApp AFF A50** 全快閃儲存陣列 |
| 配置數量 | **1 套 HA Pair**（2 個控制器） |
| 初始可用容量 | **80TB** |
| 最大擴充容量 | 最大有效容量可達 **186PB** |
| 儲存介質 | NVMe SSD (48 顆 1.92TB，原始容量 92.16TB，經 RAID-DP 後可用約 80TB) |
| 傳輸協定 | NVMe/TCP (主要), NFS (次要) |
| 效能指標 | 數百萬 IOPS, 微秒級延遲 |
| 關鍵功能 | 線上壓縮與去重、QoS、快照、加密 |

### 3.3 網路子系統 (Network Subsystem)

#### 3.3.1 網路架構：Leaf-Spine

為滿足 AI 叢集對高頻寬、低延遲、無阻塞的東西向流量需求，本方案採用業界主流的 **Leaf-Spine** 網路架構。此架構具備極佳的水平擴充性與可預測的效能，所有節點間的通訊最多只需經過一次 Spine 交換器，大幅降低延遲。

**圖 3-1：Leaf-Spine 網路拓撲圖**

![Leaf-Spine 網路拓撲圖](/home/ubuntu/leaf_spine_topology.png)

#### 3.3.2 交換器選型

**表 3-9：交換器配置總覽**

| 角色 | 推薦型號 | 數量 | 主要用途 |
|:---|:---|:---:|:---|
| Spine | **NVIDIA Spectrum-2 SN3700C** | 2 台 | 核心骨幹，提供 200GbE 高速互聯 |
| Leaf | **NVIDIA Spectrum-2 SN3420** | 3 台 | 伺服器與儲存設備接入 |

**方案優勢：**

- **無阻塞架構**：提供高達 **12.8 Tb/s** 的交換容量，確保 GPU 伺服器間的 RDMA 流量暢行無阻。
- **端到端 RoCE**：全系列支援 RoCEv2 (RDMA over Converged Ethernet)，為 NVMe-oF 與 GPU Direct 提供超低延遲的網路傳輸。
- **智慧遙測**：內建 What Just Happened (WJH) 功能，可即時監控與診斷網路異常，大幅簡化維運。

### 3.4 資安邊界子系統 (Security Perimeter Subsystem)

資安邊界子系統的設計已於 2.2.1 節詳述，此處不再贅述。

### 3.5 邊緣運算子系統 (Edge Computing Subsystem)

#### 3.5.1 邊緣運算需求

在許多業務場景中，資料具有高度的即時性與在地性。將所有資料傳回中心平台處理不僅增加網路負擔，也可能引發資安疑慮。因此，在業務單位端部署具備 AI 推論能力的邊緣運算節點，成為提升整體平台效益的關鍵。

#### 3.5.2 邊緣運算單元選型

**表 3-10：研華 MIC-743-AT 邊緣運算單元配置**

| 項目 | 規格 |
|:---|:---|
| 推薦型號 | **研華 MIC-743-AT** 工業級邊緣 AI 系統 |
| 數量 | **8 台** |
| 核心處理器 | **NVIDIA Jetson Thor T5000** SoC |
| AI 算力 | 2,070 TOPS (FP4) |
| 記憶體 | 128 GB LPDDR5X |
| 儲存 | 1TB NVMe SSD |
| 網路介面 | 1x 100GbE QSFP28, 2x 10GbE SFP+ |

**部署效益：**

- **本地知識庫**：可在邊緣端部署特定業務領域的 RAG 知識庫，實現低延遲的在地化問答。
- **即時推論**：處理來自本地端點的即時資料流（如影像、語音），執行快速推論。
- **資料預處理**：在邊緣端對原始資料進行清洗、過濾與標註，減輕中心平台的負擔。
- **提升韌性**：即使與中心平台的連線中斷，邊緣節點仍可獨立運作，提供基礎 AI 服務。

---

## 第四章 SEGMA AI 服務平台 (SaaS) 設計

在基礎設施層 (IaaS) 與服務管理層 (PaaS) 之上，本方案提供一套完整的 **SEGMA AI 服務平台 (SaaS)**，其定位不僅是 AI 應用的運行環境，更是一個賦能貴中心實現數據價值最大化的「企業級 AI 數據中台」。此平台將底層複雜的技術封裝成易於使用的模組化服務與圖形化介面，讓各業務單位的人員無需具備深厚的程式設計背景，也能快速建構、管理並應用 AI 服務。

### 4.1 平台定位：企業級 AI 數據中台

SEGMA AI 服務平台的設計理念是將其打造為貴中心的「AI 大腦」，其核心任務是**賦予大型語言模型 (LLM) 感知、決策與行動的能力**，使其能真正理解並執行財政業務領域的複雜任務。平台整合了從數據接入、知識庫構建、AI 代理人 (Agent) 設計到應用渠道發布的全鏈路功能，旨在解決純 LLM 技術無法獨立應對的挑戰，例如缺乏領域知識、無法執行外部操作、以及數據更新不及時等問題。

### 4.2 Agentic RAG 核心工作流

平台的核心是 **Agentic RAG (檢索增強生成)** 工作流，它結合了 AI Agent 的自主規劃能力與 RAG 的外部知識檢索能力。此工作流能讓 LLM 在接收到使用者請求後，自主地進行任務分解、使用工具、查詢知識庫、並最終生成精確且符合事實的答覆。

**圖 4-1：SEGMA Agentic RAG 工作流示意圖**

![SEGMA Agentic RAG 工作流示意圖](/home/ubuntu/segma_rag_workflow.png)

如上圖所示，其工作流程包含以下關鍵步驟：

1.  **任務規劃 (Planning)**：Agent 核心接收到任務後，首先進行子目標分解 (Subgoal Decomposition)，將複雜任務拆解成一系列可執行的小步驟。
2.  **工具使用 (Tool Use)**：根據任務需求，Agent 會選擇並使用預先配置的工具，例如使用「網頁爬蟲」獲取最新匯率、使用「計算機」進行稅額試算，或透過 API 呼叫內部系統查詢資料。
3.  **知識檢索 (Knowledge Retrieval)**：當任務需要特定領域知識時，Agent 會從向量資料庫中檢索最相關的文件片段，作為生成回覆的上下文依據。
4.  **記憶體 (Memory)**：Agent 具備短期與長期記憶體，能夠記住當前對話的上下文，並從過去的交互中學習。
5.  **行動生成 (Action Generation)**：在整合所有資訊後，LLM 最終生成回覆或執行相應的業務操作。

### 4.3 模組化功能設計

SEGMA AI 服務平台採用高度模組化的設計，提供彈性讓使用者根據需求選擇所需的功能模組。所有模組均提供全中文化的圖形化操作介面，大幅降低使用門檻。

**表 4-1：SEGMA AI 服務平台核心功能模組**

| 類別 | 模組名稱 | 功能描述 |
|:---|:---|:---|
| **數據整合** | **Data Studio (資料工作室)** | 提供 No-code 圖形化介面，讓使用者能輕鬆串接內部的關聯式資料庫 (如 MSSQL, Oracle)，並透過拖拉點選的方式，將結構化資料轉換為 AI 可用的標籤、名單、指標等，建立 RAG-Ready 的「資料商店」。 |
| | **Synchronization (同步作業)** | 提供 No-code 圖形化介面，讓使用者可以排程資料同步作業，確保 AI 使用的資料永遠保持在最新狀態。 |
| | **Dashboard (儀表板)** | 內建 No-code 圖形化儀表板建立工具，可快速將資料分析結果以視覺化圖表呈現。 |
| **RAG 應用** | **AI Agent (AI 代理人)** | 核心功能模組。提供視覺化拖拉式流程設計工作區，讓使用者可以設計聊天流程 (Chat Flow) 與代理人工作流 (Agent Flow)，並整合各式工具 (如搜尋引擎、計算機) 與知識庫。 |
| | **Document Loader (文件載入器)** | 提供圖形化介面，支援多種文件格式 (PDF, DOCX, PPTX, HTML, CSV 等) 的上傳、自動化切割、向量化，並存入指定的向量資料庫，建立可供 RAG 使用的知識庫。 |
| | **Feature Store (特徵商店)** | 內建記憶體內 (In-memory) 特徵商店，用於快取常用資料，加速 LLM RAG 與機器學習模型的推論速度。 |
| **應用開發** | **Streamlit APP** | 內建 Low-code 開發環境，允許開發者使用 Python 快速建立與數據或 RAG 相關的客製化 Web 應用程式。 |

### 4.4 中文化圖形化介面

平台所有核心功能均配備直觀、易用的全中文化圖形化介面 (GUI)，旨在讓非技術背景的業務專家也能深度參與 AI 應用的建構過程。

- **資料工作室 GUI**：使用者可透過點選和篩選，從複雜的資料庫中生成目標客群名單，而無需撰寫任何 SQL 程式碼。
- **文件載入器 GUI**：三步驟引導式流程（選擇嵌入模型 -> 選擇向量儲存 -> 執行），即可完成知識庫的建立。
- **代理人流程設計 GUI**：採用視覺化拖拉式畫布，使用者可以像繪製流程圖一樣，將不同的工具 (Tools)、模型 (LLM)、記憶體 (Memory) 等元件組合起來，定義出一個完整的 AI Agent 工作流程。

---

## 第五章 服務與管理層 (PaaS) 設計

服務與管理層是將底層硬體資源轉化為可用 AI 服務的關鍵。本章將深入闡述以 **SEGMA 資源管理平台** 為核心的軟體架構設計。

### 5.1 SEGMA 資源管理平台總覽

SEGMA 資源管理平台是一套專為企業級 AI 應用設計的雲原生 GPU 管理平台。它以 Kubernetes 為基礎，採用 Server-Worker 架構，提供從資源接入、調度、部署到監控的全生命週期管理能力。平台的核心目標是**最大化 GPU 資源利用率**、**簡化 AI 應用部署流程**、並**提供企業級的維運與治理能力**。

**圖 5-1：SEGMA 資源管理平台架構圖**

![SEGMA 資源管理平台架構圖](/home/ubuntu/segma_platform_architecture.png)

平台主要由六大核心模組組成：

1. **多叢集管理模組 (Multi-Cluster Management)**
2. **智慧調度器 (Intelligent Scheduler)**
3. **模型部署與服務模組 (Model Deployment & Serving)**
4. **SEGMA API Gateway**
5. **可觀測性模組 (Observability Stack)**
6. **使用者與權限管理模組 (User & Access Management)**

### 5.2 GPU 資源管理與智慧調度機制

高效的 GPU 資源管理是平台的核心競爭力。SEGMA 平台透過資源池化、多元虛擬化技術與智慧調度演算法，實現對底層異質 GPU 資源的精細化管理與高效利用。

#### 5.2.1 GPU 資源池化與多元虛擬化

平台透過 Kubernetes Device Plugin 機制，將所有伺服器節點上的 GPU（包括 H200 與 RTX 6000）無縫納管至統一的資源池中。針對不同業務場景對資源隔離性、共享性與成本效益的多元需求，平台支援以下三種主流的 GPU 虛擬化技術，並提供圖形化介面進行統一配置與管理：

**表 5-1：GPU 虛擬化技術比較與應用場景**

| 技術方案 | 原理 | 優點 | 缺點 | 適用場景 |
|:---|:---|:---|:---|:---|
| **NVIDIA MIG** | 硬體分割 | 完全的效能與故障隔離 (QoS)、安全性高 | 僅支援特定高階 GPU (本方案 H200/RTX 6000 均支援)、分割數量與單位固定 | 需要嚴格效能保證與安全隔離的多租戶推論服務、關鍵業務應用。 |
| **GPU 時間切片** | 分時共享 | 資源利用率極高、支援所有 NVIDIA GPU、靈活性高 | 無法保證效能 (QoS)、存在資源搶佔風險 | 開發測試環境、非關鍵性任務、對延遲不敏感的離線批次處理。 |
| **NVIDIA vGPU** | 軟體虛擬化 | 支援 GPU 直通與虛擬化共享、相容主流虛擬化平台 (VMware, KVM) | 需額外 vGPU 軟體授權費用 | 在現有虛擬化環境中整合 GPU 資源，提供圖形工作站 (VDI) 或 AI 運算能力。 |

#### 5.2.2 雙階段智慧調度器

為實現最佳的資源利用率與任務執行效率，SEGMA 平台內建了先進的雙階段智慧調度器。此調度器深度整合 Kubernetes，在任務提交時自動為其匹配最合適的 GPU 資源。

**第一階段：過濾 (Filtering Phase)**

當一個新的 AI 任務提交時，調度器會先對叢集中的所有 Worker 節點進行過濾，排除不符合任務需求的節點。過濾策略精細且全面，確保任務運行的基本條件得到滿足：

- **GPU 記憶體匹配**：確保節點的可用 GPU 記憶體大於任務請求值。
- **GPU 型號相容性**：若任務指定特定 GPU 型號（如 H200），則只選擇匹配的節點。
- **Worker 節點健康狀態**：自動排除處於故障或維護狀態的節點。
- **標籤選擇器 (Node Selector)**：允許使用者透過標籤指定任務運行的節點範圍（例如，`env=prod` 或 `dept=tax`）。
- **資源配額檢查 (Resource Quota)**：檢查提交任務的使用者或團隊是否還有足夠的資源配額。
- **親和性/反親和性規則 (Affinity/Anti-Affinity)**：根據規則將特定任務部署在相同或不同的節點上，以優化效能或提升可用性。

**第二階段：評分 (Scoring Phase)**

通過過濾階段後，調度器會對所有候選節點進行評分，選出最優節點。平台支援兩種核心評分策略，管理者可根據營運目標進行選擇：

- **Binpack (集中式)**：優先選擇已被佔用最多資源的節點。此策略傾向於將任務集中在少數節點上，讓其他節點保持空閒，以便運行需要大量資源的大型任務，同時有助於在離峰時段關閉閒置節點以節省電力。
- **Spread (分散式)**：優先選擇最空閒的節點。此策略傾向於將任務均勻分散到所有節點，當某個節點意外故障時，對整體服務的影響最小，服務可用性最高。
### 5.3 模型部署與高效推論引擎

SEGMA 平台極大簡化了 AI 模型的生命週期管理，從模型上線、服務到監控，提供一站式的解決方案，並透過整合多種業界頂尖的推論引擎，確保最佳的服務效能。

#### 5.3.1 一鍵式模型部署與生命週期管理

平台提供全圖形化的模型管理介面，讓模型上線從數天縮短至數分鐘：

- **多來源匯入**：支援從 **Hugging Face**、**ModelScope** 等主流模型社群直接匯入模型，或從本地上傳模型檔案及完整目錄。
- **模型版本控制**：自動管理模型的不同版本，支援灰度發布與快速回滾。
- **副本數動態調整**：可根據即時負載，手動或自動調整模型服務的副本數量 (Replicas)，實現彈性擴展。
- **離線部署支援**：支援在 Air-Gapped 封閉式環境下，透過上傳預先打包的模型檔案進行部署，完全滿足貴中心的資安要求。

#### 5.3.2 多推論引擎原生支援

為了最大化不同模型在異質 GPU 上的推論效率，平台原生整合了多種業界頂尖的推論引擎。平台會根據模型架構、大小與硬體特性，自動推薦或選擇最適合的引擎來運行推論服務。

**表 5-2：支援推論引擎與技術特性**

| 推論引擎 | 核心技術 | 主要優勢 | 適用場景 |
|:---|:---|:---|:---|
| **vLLM** | PagedAttention, Continuous Batching | 高吞吐量、記憶體效率高 | 大規模、高併發的 LLM 推論服務。 |
| **SGLang** | RadixAttention, Compressed FSM | 極致的並行處理能力、低延遲 | 對回應延遲要求極高的即時交互應用。 |
| **TensorRT-LLM** | NVIDIA 官方優化 | 充分發揮 NVIDIA GPU 硬體效能、支援最新架構 | 追求極致硬體效能、需要確定性延遲的場景。 |
| **Ollama / llama.cpp** | 量化、CPU/GPU 混合推論 | 資源佔用低、部署簡單快速 | 在邊緣運算單元或資源有限的環境中運行輕量級模型。 |

#### 5.3.3 多 GPU 分散式推論

對於超出單張 GPU 記憶體容量的超大型模型，平台支援多種分散式推論策略，可將單一模型拆分至多張甚至多台伺服器的 GPU 上協同運行：

- **Tensor Parallelism (TP)**：將模型的權重矩陣分割到多個 GPU 上並行計算，適用於 Transformer 中的大型矩陣乘法運算。
- **Pipeline Parallelism (PP)**：將模型的不同層 (Layers) 分配到不同的 GPU 上，形成一個流水線，提升處理效率。
- **Data Parallelism (DP)**：在多個 GPU 上各自運行完整的模型副本，同時處理不同的請求，以提升整體吞吐量。### 5.4 SEGMA API Gateway：統一模型服務供應

SEGMA API Gateway 是平台統一的服務供應層，它將底層多樣化的雲端與地端模型，以及複雜的路由、負載均衡、安全策略，全部封裝成一個對開發者極其友善的單一入口。其設計理念源於業界成熟的 API Gateway 模式，並針對 LLM 服務的獨特性進行了深度優化，可視為專為大型語言模型打造的「服務供應管理平台」。

#### 5.4.1 統一 OpenAI 相容 API

API Gateway 的核心價值在於提供**統一的服務窗口**。無論後端模型來自 OpenAI、Google、Anthropic，或是部署在地端 H200 上的開源模型，對應用程式開發者而言，都只有一個統一、標準且相容 OpenAI 的 API 端點。這帶來了巨大的好處：
- **簡化開發**：開發者無需為每種模型學習不同的 API 格式，大幅降低學習成本與開發複雜度。
- **無縫切換**：可以隨時在後端更換或增加模型，而前端應用程式的程式碼**完全無需修改**。
- **未來擴充性**：未來若需引入新的模型，只需在 Gateway 層進行配置，即可立即對所有應用程式提供服務。

#### 5.4.2 智慧路由與負載均衡

API Gateway 內建強大的智慧路由引擎，可根據預設規則自動將請求分發至最適當的模型部署實例：

- **多種路由策略**：支援 `simple-shuffle` (隨機輪詢)、`least-busy` (最少請求)、`latency-based` (最低延遲)、`cost-based` (最低成本) 等多種策略。
- **模型備援 (Fallback)**：可設定備援鏈。例如，當主要的地端 Llama 3.1 模型因負載過高或故障而無法回應時，Gateway 會自動將請求轉發至備援的 RTX 6000 節點，甚至在緊急情況下路由至預先配置的雲端 API，確保服務的最高可用性。
- **A/B 測試**：支援流量鏡像 (Traffic Mirroring) 功能，可將部分生產流量複製到新的模型版本進行測試，以便在不影響線上服務的情況下評估新模型的效能與準確性。

#### 5.4.3 企業級安全與治理

針對企業與政府機關對安全與治理的嚴格要求，API Gateway 提供了完善的管理機制：

- **虛擬金鑰管理**：管理者可以為不同的部門、專案或使用者生成獨立的**虛擬 API Key**。這些金鑰與後端模型的真實金鑰完全隔離，即使虛擬金鑰外洩，真實金鑰也安然無恙。
- **多層級預算與配額管理**：可針對每個虛擬金鑰設定精細的**速率限制 (RPM/TPM)** 與**用量配額 (每日/每月 Token 上限)**。此功能可有效防止資源濫用，並實現精準的成本分攤。
- **用量追蹤與成本分析**：平台會自動追蹤每個虛擬金鑰的 Token 使用量，並根據預設的模型價格即時計算成本。所有數據均可透過儀表板呈現，並支援匯出報表，為預算規劃提供數據支持。
- **安全護欄 (Guardrails)**：可設定全域的規則，例如強制在所有 Prompt 前加入特定的系統提示 (System Prompt)，或對請求與回覆進行 PII (個人可識別資訊) 脫敏處理，確保所有 AI 交互均符合資安規範。

### 5.5 系統監控與可觀測性

平台內建完整的可觀測性解決方案，提供從硬體到應用的全維度監控：

- **GPU 指標監控**：透過 **NVIDIA DCGM (Data Center GPU Manager)** 蒐集超過 300 項詳細的 GPU 指標，包括 GPU 利用率、記憶體使用率、溫度、功耗、NVLink 流量、PCIe 流量等。
- **LLM 推論指標監控**：針對 LLM 服務，提供關鍵效能指標 (KPI) 監控，如 **Time to First Token (TTFT)**、**Inter-Token Latency (ITL)**、**End-to-End Latency**、**Throughput (Tokens/sec)** 等。
- **平台與叢集監控**：整合 **Prometheus** 監控 Kubernetes 叢集狀態、節點資源使用率、Pod 運行狀態等。
- **日誌聚合與查詢**：整合 **Loki** 聚合所有系統與應用的日誌，提供統一的查詢介面。
- **Grafana 儀表板**：提供多個預設的 Grafana 儀表板，將上述所有監控指標以視覺化的方式呈現，並支援自訂告警規則。

### 5.6 使用者、權限與叢集管理

- **多租戶管理**：支援建立多個租戶 (Tenant)，每個租戶擁有獨立的資源配額、使用者群組與模型服務，實現資源的邏輯隔離。
- **角色型權限控制 (RBAC)**：預設多種系統角色（如管理員、開發者、訪客），並支援自訂角色與權限，可精細控制使用者對不同功能模組與資源的存取權限。
- **SSO 整合**：支援與企業現有的 **Active Directory / LDAP** 進行整合，實現單一簽入 (Single Sign-On)。
- **多叢集納管**：平台不僅能管理本地 GPU 叢集，未來還可透過標準 Kubernetes API 納管位於公有雲或不同地機房的 GPU 叢集，實現混合雲資源的統一視圖與管理。

---

## 第六章 專案管理與執行規劃

### 6.1 專案建置時程規劃

本專案從簽約、設備採購、系統建置、測試驗收到正式上線，預計總時程為 **6 個月**。詳細時程規劃如下：

**表 6-1：專案時程規劃 (Gantt Chart)**

| 階段 | 月份 1 | 月份 2 | 月份 3 | 月份 4 | 月份 5 | 月份 6 |
|:---|:---:|:---:|:---:|:---:|:---:|:---:|
| **1. 專案啟動與採購** | ████ | | | | | |
| **2. 硬體建置與安裝** | | ██████ | ████ | | | |
| **3. 軟體平台部署** | | | ██████ | ████ | | |
| **4. 系統整合與測試** | | | | ██████ | ████ | |
| **5. 教育訓練與上線** | | | | | | ██████ |

### 6.2 預算成本估算

本專案總預算約為新台幣 **6,217 萬元**，詳細成本結構如下表所示。此估算已包含所有硬體、軟體授權、建置服務、以及第一年的維運保固費用。

**表 6-2：預算成本估算表**

| 項目 | 細項 | 單位 | 數量 | 單價 (萬) | 總價 (萬) | 備註 |
|:---|:---|:---:|:---:|:---:|:---:|:---|
| **硬體** | H200 運算伺服器 | 台 | 3 | 500 | 1,500 | 含 4x H200 GPU |
| | RTX 6000 運算伺服器 (4-GPU) | 台 | 1 | 600 | 600 | 含 4x RTX 6000 GPU |
| | RTX 6000 運算伺服器 (2-GPU) | 台 | 1 | 320 | 320 | 含 2x RTX 6000 GPU |
| | NetApp AFF A50 儲存陣列 | 套 | 1 | 450 | 450 | 80TB 可用空間 |
| | NVIDIA SN3700C 交換器 | 台 | 2 | 85 | 170 | Spine 交換器 |
| | NVIDIA SN3420 交換器 | 台 | 3 | 45 | 135 | Leaf 交換器 |
| | Palo Alto PA-5450 防火牆 | 套 | 1 | 60 | 60 | HA Pair |
| | A10 Thunder ADC | 套 | 1 | 30 | 30 | HA Pair |
| | 研華 MIC-743-AT 邊緣運算單元 | 台 | 8 | 70 | 560 | 含 Jetson Thor T5000 |
| | 機櫃與附屬設備 | 式 | 1 | 240 | 240 | PDU, KVM, 線材等 |
| **軟體** | SEGMA AI 服務平台 (SaaS) | 套 | 1 | 400 | 400 | 永久授權 |
| | SEGMA 資源管理平台 (PaaS) | 套 | 1 | 600 | 600 | 永久授權 |
| | 作業系統與資料庫 | 式 | 1 | 32 | 32 | RHEL, Windows Server, Milvus 等 |
| **服務** | 系統建置與整合服務 | 式 | 1 | 800 | 800 | |
| | 教育訓練 | 式 | 1 | 80 | 80 | |
| | 第一年維運保固 | 式 | 1 | 240 | 240 | |
| **合計** | | | | | **6,217** | |

### 6.3 風險評估與應對策略

**表 6-3：風險評估與應對策略**

| 風險類別 | 風險描述 | 可能性 | 衝擊性 | 應對策略 |
|:---|:---|:---:|:---:|:---|
| **技術風險** | GPU 硬體交貨延遲 | 中 | 高 | 提前下訂，並與多家供應商建立合作關係。 |
| | 軟體平台與硬體相容性問題 | 低 | 中 | 採用主流大廠硬體，並在測試環境進行完整相容性驗證。 |
| **管理風險** | 專案需求變更 | 高 | 中 | 建立完整的需求變更管理流程，所有變更需經雙方確認。 |
| **環境風險** | 機房電力或散熱不符預期 | 低 | 高 | 在建置前進行詳盡的現場勘查與壓力測試。 |

---

## 第七章 結論與展望

本文件針對財政部財政資訊中心的「智慧算力共用平臺建置委外服務案」，提出了一套完整、先進且具前瞻性的全棧式 AI 平台解決方案。從硬體層的混合 GPU 配置、高效能網路與儲存，到軟體層的 SEGMA 資源管理平台，再到創新的邊緣運算架構，每一項設計都旨在為貴中心打造一個安全、高效、可控的 AI 基礎設施。

我們相信，本方案不僅能滿足當前的 AI 算力需求，更能作為貴中心未來發展各項智慧應用的堅實後盾，助力財政部在數位轉型的浪潮中，持續引領創新。云碩科技期待能有機會與貴中心攜手合作，共同實現此一宏偉藍圖。

---

## 附錄 A：伺服器詳細配置表

**表 A-1：H200 運算伺服器詳細配置 (共 3 台)**

| 組件 | 型號與規格 |
|:---|:---|
| **機箱** | HPE ProLiant DL380a Gen12 (4U) |
| **CPU** | 2x Intel® Xeon® Platinum 6745P (32 Cores, 2.4 GHz) |
| **記憶體** | 1.5 TB DDR5 5600MHz (24x 64GB) |
| **GPU** | 4x NVIDIA H200 NVL 141GB PCIe |
| **網路卡 1** | 1x NVIDIA ConnectX-7 200GbE 雙埠 QSFP112 (for GPU) |
| **網路卡 2** | 1x NVIDIA ConnectX-6 Lx 25GbE 雙埠 SFP28 (for Mgmt/Storage) |
| **網路卡 3** | 1x NVIDIA ConnectX-6 Lx 25GbE 雙埠 SFP28 OCP 3.0 |
| **儲存** | 2x 1.92TB NVMe U.3 SSD (RAID 1, for OS) |
| **電源** | 4x 2200W (3+1 冗餘) |

**表 A-2：RTX 6000 運算伺服器 (4-GPU) 詳細配置 (共 1 台)**

| 組件 | 型號與規格 |
|:---|:---|
| **機箱** | HPE ProLiant DL380a Gen12 (4U) |
| **CPU** | 2x Intel® Xeon® Platinum 6745P (32 Cores, 2.4 GHz) |
| **記憶體** | 1.5 TB DDR5 5600MHz (24x 64GB) |
| **GPU** | 4x NVIDIA RTX 6000 Pro Server Blackwell Edition 96GB |
| **網路卡 1** | 1x NVIDIA ConnectX-7 200GbE 雙埠 QSFP112 (for GPU) |
| **網路卡 2** | 1x NVIDIA ConnectX-6 Lx 25GbE 雙埠 SFP28 (for Mgmt/Storage) |
| **網路卡 3** | 1x NVIDIA ConnectX-6 Lx 25GbE 雙埠 SFP28 OCP 3.0 |
| **儲存** | 2x 1.92TB NVMe U.3 SSD (RAID 1, for OS) |
| **電源** | 4x 2200W (3+1 冗餘) |

**表 A-3：RTX 6000 運算伺服器 (2-GPU) 詳細配置 (共 1 台)**

| 組件 | 型號與規格 |
|:---|:---|
| **機箱** | HPE ProLiant DL380a Gen12 (4U) |
| **CPU** | 2x Intel® Xeon® Gold 6527P (20 Cores, 2.8 GHz) |
| **記憶體** | 512 GB DDR5 5600MHz (16x 32GB) |
| **GPU** | 2x NVIDIA RTX 6000 Pro Server Blackwell Edition 96GB |
| **網路卡 1** | 1x NVIDIA ConnectX-6 Dx 100GbE 雙埠 QSFP56 (for GPU) |
| **網路卡 2** | 1x NVIDIA ConnectX-6 Lx 25GbE 雙埠 SFP28 OCP 3.0 (for Mgmt/Storage) |
| **儲存** | 2x 1.92TB NVMe U.3 SSD (RAID 1, for OS) |
| **電源** | 2x 2200W (1+1 冗餘) |

---

## 參考文獻

1. NVIDIA. (2024). *NVIDIA H200 Tensor Core GPU*. [https://www.nvidia.com/en-us/data-center/h200/](https://www.nvidia.com/en-us/data-center/h200/)
2. NVIDIA. (2024). *NVIDIA RTX 6000 Ada Generation*. [https://www.nvidia.com/en-us/design-visualization/rtx-6000/](https://www.nvidia.com/en-us/design-visualization/rtx-6000/)
3. HPE. (2024). *HPE ProLiant DL380a Gen12 Server*. [https://www.hpe.com/psnow/doc/psnow-a00133064enw](https://www.hpe.com/psnow/doc/psnow-a00133064enw)
4. NetApp. (2024). *AFF A-Series All-Flash Arrays*. [https://www.netapp.com/data-storage/all-flash-array/aff-a-series/](https://www.netapp.com/data-storage/all-flash-array/aff-a-series/)
5. NVIDIA. (2024). *NVIDIA Spectrum-2 Switches*. [https://www.nvidia.com/en-us/networking/ethernet-switching/spectrum-2/](https://www.nvidia.com/en-us/networking/ethernet-switching/spectrum-2/)
6. Advantech. (2024). *MIC-743-AT, Industrial Edge AI System*. [https://www.advantech.com/en/products/60d3c63b-5a3e-4b8f-9b9b-2049b8f9c1de/mic-743-at/mod_ae0a0c6a-9b1d-4b8c-8c9e-0e6d6f5e4f3c](https://www.advantech.com/en/products/60d3c63b-5a3e-4b8f-9b9b-2049b8f9c1de/mic-743-at/mod_ae0a0c6a-9b1d-4b8c-8c9e-0e6d6f5e4f3c)
7. Kubernetes. (2024). *Device Plugins*. [https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)
8. vLLM Project. (2024). *vLLM: Easy, fast, and cheap LLM serving*. [https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)
9. SGLang Team. (2024). *SGLang: Efficient Language and System for LLMs*. [https://github.com/sgl-project/sglang](https://github.com/sgl-project/sglang)
